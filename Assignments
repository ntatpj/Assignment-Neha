Assignment 1:

Technically the difference between RC and Replaics set is the “matchlabel” selector type.
In RC we have only one selector i.e app:kubia , so system will acquired all POD labels “app;kubia”
However in Repliacset we have “matchlables”. Matchlabels tells what PODs deployment will be applied to. It will not apply on all pods creted with same image, instead it will be selective based on matchlabel which PODs have same labels, only on those POD it will select and work. 


•	TO use kubectl apply:
  kubectl apply (-f FILENAME | -k DIRECTORY) [options]
kubectl apply -f will be used when we create PODs with help of a file.
[root@ip-172-31-28-61 05-services]# kubectl apply -f kubia-replicaset.yaml
replicaset.apps/kubia created
•	Access POD with POD ip and port 

 



To create service :

[root@ip-172-31-28-61 05-services]# kubectl apply -f kubia-svc.yaml
service/kubia2 created
[root@ip-172-31-28-61 05-services]# kubectl get svc
NAME         TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1     <none>        443/TCP   2m7s
kubia2       ClusterIP   10.99.10.99   <none>        80/TCP    7s
[root@ip-172-31-28-61 05-services]#





•	Kubia svc will attach service only to pods name: kubia
[root@ip-172-31-28-61 05-services]# cat kubia-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia2
spec:
  clusterIP: 10.99.10.99
  ports:
  - port: 80
    targetPort: 8080
  selector:
      app: kubia
[root@ip-172-31-28-61 05-services]#

•	Since the PODS created with sepc app:kubia by replicaset.yaml file, serive will be atched to those PODs as selector is label app:kubia in kubia-svc.yaml.

[root@ip-172-31-28-61 05-services]# cat kubia-replicaset.yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia

[root@ip-172-31-28-61 05-services]# kubectl get po --show-labels
NAME          READY   STATUS    RESTARTS   AGE   LABELS
kubia-7pgq8   1/1     Running   0          41m   app=kubia
kubia-drnwd   1/1     Running   0          41m   app=kubia
kubia-khzc9   1/1     Running   0          41m   app=kubia
•	Now the service will inturn contact a POD. Each time it will select different POD created by same replicaset randomly.
NAME         TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1     <none>        443/TCP   9m43s
kubia2       ClusterIP   10.99.10.99   <none>        80/TCP    7m43s
[root@ip-172-31-28-61 05-services]# curl 10.99.10.99
You've hit kubia-khzc9

[root@ip-172-31-28-61 05-services]# kubectl get po
NAME          READY   STATUS    RESTARTS   AGE
kubia-7pgq8   1/1     Running   0          39m
kubia-drnwd   1/1     Running   0          39m
kubia-khzc9   1/1     Running   0          39m
[root@ip-172-31-28-61 05-services]#



•	I changed the label from kubia to neha. Observed a new replaicaset POD was creted instead. Also the curl operation was not able to access the POD whose label was changed.
[root@ip-172-31-28-61 05-services]# kubectl get po --show-labels
NAME          READY   STATUS    RESTARTS   AGE   LABELS
kubia-7pgq8   1/1     Running   0          41m   app=kubia
kubia-drnwd   1/1     Running   0          41m   app=kubia
kubia-khzc9   1/1     Running   0          41m   app=kubia
[root@ip-172-31-28-61 05-services]# kubectl label po kubia-7pgq8 app=neha --overwrite
pod/kubia-7pgq8 labeled
[root@ip-172-31-28-61 05-services]# curl 10.99.10.99
You've hit kubia-drnwd
[root@ip-172-31-28-61 05-services]#
[root@ip-172-31-28-61 05-services]# curl 10.99.10.99
You've hit kubia-drnwd
[root@ip-172-31-28-61 05-services]# curl 10.99.10.99
You've hit kubia-drnwd
[root@ip-172-31-28-61 05-services]# kubectl get po --show-labels
NAME          READY   STATUS    RESTARTS   AGE   LABELS
kubia-7pgq8   1/1     Running   0          44m   app=neha
kubia-drnwd   1/1     Running   0          44m   app=kubia
kubia-khzc9   1/1     Running   0          44m   app=kubia
kubia-v9xbc   1/1     Running   0          34s   app=kubia
[root@ip-172-31-28-61 05-services]# curl 10.99.10.99
You've hit kubia-v9xbc
[root@ip-172-31-28-61 05-services]# curl 10.99.10.99
You've hit kubia-drnwd
[root@ip-172-31-28-61 05-services]# curl 10.99.10.99
You've hit kubia-drnwd
[root@ip-172-31-28-61 05-services]#









-------------------------------------------------
-------------------------------------------------
-------------------------------------------------
-------------------------------------------------
-------------------------------------------------




   Have run below commands during assignment 3.
   
•	cd k8s-specifications/
•	ls
•	kubectl apply -f .
•	kubcectl get -all
•	kubcectl get all
•	kubectl get all
•	ll
•	kubectl get po
NAME                      READY   STATUS    RESTARTS   AGE
db-b54cd94f4-gp2d2        1/1     Running   0          22m
redis-868d64d78-dsgxr     1/1     Running   0          22m
result-5d57b59f4b-rhh5n   1/1     Running   0          22m
vote-94849dc97-7x8zk      1/1     Running   0          22m
worker-dd46d7584-v69c6    1/1     Running   0          22m

•	kubectl delete po vote-94849dc97-7x8zk                        (Observed a new voter POD was created, the functionality of the system was not affected, the dynamic changes in votes was reflected)
•	kubectl delete po worker-dd46d7584-v69c6  					  (Observed a new worker POD was created, the functionality of the system was not affected, the dunamic changes in votes was reflected)
•	kubectl get po
•	kubectl delete po db-b54cd94f4-gp2d2  		(Observed a new DB POD was created, however the dynamic changes in the vote input was not reflected. Reason : This is because result app was querying the older DB POD and socket connection was established with it. Now since the older DB is not existing there is no other end point for this socket connection
•	The results are updated only when the result.app POD is deleted and recreated. Basically recreation lead to establishment of new socket connection and hence now the DB POD and result POD are connected.
•	To delete result POD :kubectl delete po result-5d57b59f4b-rhh5n   
													       (Basically all the deleted PODs, be it vote, worker or db all were recreted on deletion)
	
	[root@ip-172-31-28-61 k8s-specifications]# kubectl get po
NAME                      READY   STATUS    RESTARTS   AGE
db-b54cd94f4-8mxd7        1/1     Running   0          105s
redis-868d64d78-dsgxr     1/1     Running   0          28m
result-5d57b59f4b-rhh5n   1/1     Running   0          28m
vote-94849dc97-p5qpq      1/1     Running   0          5m51s
worker-dd46d7584-lgqx2    1/1     Running   1          3m35s

	
    















<Assignment2>

[root@ip-172-31-28-61 04-controllers]# ls
batch-job                                kubia-rc.yaml                           multi-completion-parallel-batch-job.yaml
batch-job.yaml                           kubia-replicaset-matchexpressions.yaml  ssd-monitor
cronjob.yaml                             kubia-replicaset.yaml                   ssd-monitor-daemonset.yaml
kubia-liveness-probe-initial-delay.yaml  kubia-unhealthy                         time-limited-batch-job.yaml
kubia-liveness-probe.yaml                multi-completion-batch-job.yaml
[root@ip-172-31-28-61 04-controllers]# vi cronjob.yaml
[root@ip-172-31-28-61 04-controllers]# kubectl apply -f cronjob.yaml
cronjob.batch/batch-job-every-fifteen-minutes created
[root@ip-172-31-28-61 04-controllers]# kubectl get jobs
NAME                          COMPLETIONS   DURATION   AGE
multi-completion-batch-job    5/5           6m16s      143m
multi-completion-batch-job1   5/5           6m13s      43m
multi-completion-batch-job2   5/5           6m14s      21m
[root@ip-172-31-28-61 04-controllers]# kubectl get po
NAME                                READY   STATUS      RESTARTS   AGE
kubia-manual                        1/1     Running     0          2d16h
mangz-ngnix                         1/1     Running     0          4d23h
multi-completion-batch-job-27zj8    0/1     Completed   0          141m
multi-completion-batch-job-96s9m    0/1     Completed   0          143m
multi-completion-batch-job-hhzkn    0/1     Completed   0          139m
multi-completion-batch-job-rm9qw    0/1     Completed   0          143m
multi-completion-batch-job-xf68n    0/1     Completed   0          141m
multi-completion-batch-job1-b54vz   0/1     Completed   0          43m
multi-completion-batch-job1-lwmxx   0/1     Completed   0          41m
multi-completion-batch-job1-n9t7s   0/1     Completed   0          41m
multi-completion-batch-job1-v858n   0/1     Completed   0          39m
multi-completion-batch-job1-w4m25   0/1     Completed   0          43m
multi-completion-batch-job2-dwcxf   0/1     Completed   0          19m
multi-completion-batch-job2-mvttl   0/1     Completed   0          22m
multi-completion-batch-job2-vstj5   0/1     Completed   0          19m
multi-completion-batch-job2-vxv58   0/1     Completed   0          22m
multi-completion-batch-job2-wnzcr   0/1     Completed   0          17m
neha-nginx                          1/1     Running     0          3d4h
[root@ip-172-31-28-61 04-controllers]# kubectl get jobs
NAME                          COMPLETIONS   DURATION   AGE
multi-completion-batch-job    5/5           6m16s      143m
multi-completion-batch-job1   5/5           6m13s      43m
multi-completion-batch-job2   5/5           6m14s      22m
[root@ip-172-31-28-61 04-controllers]# kubectl get jobs
NAME                                         COMPLETIONS   DURATION   AGE
batch-job-every-fifteen-minutes-1656847800   1/1           2m5s       9m37s
multi-completion-batch-job                   5/5           6m16s      161m
multi-completion-batch-job1                  5/5           6m13s      61m
multi-completion-batch-job2                  5/5           6m14s      40m
[root@ip-172-31-28-61 04-controllers]#



There is no option to stop the CRON JOB, unless we delete the CRON JOB.


CRON works on some frequency of time which the user decides. Howver the parallel JOB will start running the moment when started.












-------------------------------------------------
-------------------------------------------------
-------------------------------------------------
-------------------------------------------------
-------------------------------------------------
